---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi, I am an LLM Algorithm Engineer at Huawei Noah's Ark Lab. I joined Huawei Noah's Ark Recommendation and Search Laboratory in 2022 after receiving my M.S. degree from Peking University. I primarily work on enhancing large language models' code competition capabilities and RAG (Retrieval-Augmented Generation) functionalities. Additionally, I have contributed to LLM4Rec (Large Language Models for Recommendation) projects.

My main research interests include LLM for Rec, LLM for Code, RAG and Recommender Systems.  Currently, I'm focused on leveraging Large Language Models to solve code problems and RAG.


# ğŸ”¥ News
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ Four of our papers have been accepted by ACL 2025.

- *2025.04*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by WWW 2025 .

- *2025.01*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by Coling 2025.
 
- *2024.11*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by EMNLP 2024. 


# ğŸ“ Highlighted Publications 

- **[CoIR: A Comprehensive Benchmark for Code Information Retrieval Models](https://arxiv.org/abs/2407.02883)**  
  Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, Ruiming Tang  
  *ACL 2025*

- **[CTRL: Connect Collaborative and Language Model for CTR Prediction](https://dl.acm.org/doi/abs/10.1145/3713080)**  
  Xiangyang Li, Bo Chen, Lu Hou, Ruiming Tang  
  *TORS*

- **[LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start Recommendations](https://arxiv.org/abs/2404.00702)**  
  Wenlin Zhang, Chuhan Wu, Xiangyang Li, Yuhao Wang, Kuicai Dong, Yichao Wang, Xinyi Dai, Xiangyu Zhao, Huifeng Guo, Ruiming Tang  
  *Coling 2025 (ORAL)*

- **[A Unified Framework for Multi-Domain CTR Prediction via Large Language Models](https://dl.acm.org/doi/abs/10.1145/3698878)**  
  Zichuan Fu, Xiangyang Li, Chuhan Wu, Yichao Wang, Kuicai Dong, Xiangyu Zhao, Mengchen Zhao, Huifeng Guo, Ruiming Tang  
  *TOIS *

- **[IntTower: The Next Generation of Two-Tower Model for Pre-Ranking System](https://arxiv.org/abs/2210.09890)**  
  Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long, Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, Jinxing Liu, Zhenhua Dong, Ruiming Tang  
  *CIKM 2022 (ORAL)*

- **[Low Resource Style Transfer via Domain Adaptive Meta Learning](https://arxiv.org/abs/2205.12475)**  
  Xiangyang Li, Xiang Long, Yu Xia, Sujian Li  
  *NAACL 2022 (ORAL)*




# ğŸ– Honors and Awards 
- **2019-2022** Outstanding Student Award
- **2015-2019** National Inspirational Scholarship
- **2015-2019** First-Class Scholarship

# ğŸ“– Education
- *2019.09 - 2022.06*, Peking University
- *2015.09 - 2019.06*, Nanjing University of Posts and Telecommunications
  - Exchange student at Nanjing University (Fall 2017, one semester)


# ğŸ’¬ Invited Talks
- **2023** Invited to Tsinghua University to present research on CTRL

# ğŸ’» Work Experience
- *2022.01 - Present*, **HuaWei Noah's Ark Lab**, Beijing.
  Working in the Recommendation and Search Laboratory, I have been involved in coarse ranking and search projects. Later, I participated in the development of the Pangu Large Language Model, primarily responsible for **LLM code competition capabilities** and **RAG** (Retrieval-Augmented Generation) capabilities.

## ğŸ“ Internships
- *2021.06 - 2021.09*, **MeiTuan** (Internship), Beijing.
  I was responsible for the construction and training of the coarse rank model of MeiTuan food channel. During the internship process, I screened out 20 features and used the XGBoost model to construct the coarse rank model based on CVR. Finally, we achieved an AUC score of 0.89 in production, and OPM +1.7% on the channel. The model was fully applied and effectively alleviated system stress.

- *2021.01 - 2021.04*, **JingDong** (Internship), Beijing.
  During the internship, I used the CEM optimization algorithm to regulate the distribution of traffic in JingDong main search according to online feedback, ensuring the promotion of merchants' traffic while simultaneously guaranteeing smooth growth of overall GMV.
