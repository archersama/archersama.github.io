---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I received the B.S. degree from the Nanjing University of Posts and Telecommunications, in 2019, and the M.S. degree from Peking University, in 2022. I am currently a researcher with the AI Research Institute, HuaWei Noah Ark Lab, China. My main research interests include Machine Learning, Deep Learning and Recommender Systems. Also, I do some research about Nature Language Processing, such as text classification, and text generation. Now,I focus on using LLM to solve recommendation problems.


# ğŸ”¥ News
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ Four of our papers have been accepted by ACL 2025.

- *2025.04*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by WWW 2025 .

- *2025.01*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by Coling 2025.
 
- *2024.11*: &nbsp;ğŸ‰ğŸ‰ one of our papers have been accepted by EMNLP 2024. 


# ğŸ“ Highlighted Publications 

- **[CoIR: A Comprehensive Benchmark for Code Information Retrieval Models](https://arxiv.org/abs/2407.02883)**  
  Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, Ruiming Tang  
  *ACL 2025*

- **[CTRL: Connect Collaborative and Language Model for CTR Prediction](https://dl.acm.org/doi/abs/10.1145/3713080)**  
  Xiangyang Li, Bo Chen, Lu Hou, Ruiming Tang  
  *TORS*

- **[LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start Recommendations](https://arxiv.org/abs/2404.00702)**  
  Wenlin Zhang, Chuhan Wu, Xiangyang Li, Yuhao Wang, Kuicai Dong, Yichao Wang, Xinyi Dai, Xiangyu Zhao, Huifeng Guo, Ruiming Tang  
  *Coling 2025 (ORAL)*

- **[A Unified Framework for Multi-Domain CTR Prediction via Large Language Models](https://dl.acm.org/doi/abs/10.1145/3698878)**  
  Zichuan Fu, Xiangyang Li, Chuhan Wu, Yichao Wang, Kuicai Dong, Xiangyu Zhao, Mengchen Zhao, Huifeng Guo, Ruiming Tang  
  *TOIS *

- **[IntTower: The Next Generation of Two-Tower Model for Pre-Ranking System](https://arxiv.org/abs/2210.09890)**  
  Xiangyang Li, Bo Chen, HuiFeng Guo, Jingjie Li, Chenxu Zhu, Xiang Long, Sujian Li, Yichao Wang, Wei Guo, Longxia Mao, Jinxing Liu, Zhenhua Dong, Ruiming Tang  
  *CIKM 2022 (ORAL)*

- **[Low Resource Style Transfer via Domain Adaptive Meta Learning](https://arxiv.org/abs/2205.12475)**  
  Xiangyang Li, Xiang Long, Yu Xia, Sujian Li  
  *NAACL 2022 (ORAL)*




# ğŸ– Honors and Awards 
- **2019-2022** Outstanding Student Award
- **2015-2019** National Inspirational Scholarship
- **2015-2019** First-Class Scholarship

# ğŸ“– Education
- *2019.09 - 2022.06*, Peking University
- *2015.09 - 2019.06*, Nanjing University of Posts and Telecommunications
  - Exchange student at Nanjing University (Fall 2017, one semester)


# ğŸ’¬ Invited Talks
- **2023** Invited to Tsinghua University to present research on CTRL

# ğŸ’» Internships
- *2022.01 - 2022.04*, HuaWei , Beijing.
- *2021.06 - 2020.09*, MeiTuan , Beijing.
I was responsible for the construction and training of the coarse rank model of MeiTuan food channel, during the internship process, 20 features were screened out, and the xgboost model was used to construct the coarse rank model based on CVR.Finally, we got auc score of 0.89Â  on the line, and opm + 1.7% on the channel, the modelÂ  had been fully applied, and effectively alleviated the stress of the system.
- *2021.01 - 2020.04*, JingDong , Beijing.
 During the internship process, I used the CEM optimization algorithm to regulate the distribution of flow in Jindong main search according to the feedback on the line, ensuring the promotion of merchants' flow while simultaneously and guaranteeing a smooth growth of overall gmv.
